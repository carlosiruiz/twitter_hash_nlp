{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T19:37:39.790953Z",
     "start_time": "2021-02-18T19:37:37.598403Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T19:37:39.801122Z",
     "start_time": "2021-02-18T19:37:39.794033Z"
    }
   },
   "outputs": [],
   "source": [
    "def hashtag_webscrape(hashtag:str,language:str,limit=100000,clean=True, auto_update= True):\n",
    "    import twint\n",
    "    import pandas as pd\n",
    "    import nest_asyncio\n",
    "    \n",
    "    nest_asyncio.apply()\n",
    "    c = twint.Config()\n",
    "    c.Search = hashtag\n",
    "    c.Limit = limit\n",
    "    c.Hide_output = True\n",
    "    c.Pandas = True\n",
    "    c.Pandas_clean = clean\n",
    "    c.Pandas_au = auto_update\n",
    "    \n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    tweets = twint.storage.panda.Tweets_df\n",
    "    \n",
    "\n",
    "    df = tweets[tweets.language == language]\n",
    "    df = tweets[tweets.retweet == False]\n",
    "    df = df[['tweet','hashtags']]\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "\n",
    "    print(\"Finished: Successfully collected \"+str(df.shape[0]) +\" Tweets.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T19:37:41.588273Z",
     "start_time": "2021-02-18T19:37:39.804034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Successfully collected 89 Tweets.\n"
     ]
    }
   ],
   "source": [
    "df=hashtag_webscrape(\"#women\",\"en\", limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T19:37:41.600833Z",
     "start_time": "2021-02-18T19:37:41.591529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet', 'hashtags'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T19:37:41.618587Z",
     "start_time": "2021-02-18T19:37:41.602990Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Celebrating #BlackHistoryMonth  sculptor Eliza...</td>\n",
       "      <td>[blackhistorymonth, black, women, artist, blm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Cannabis use mitigates symptoms of endometrio...</td>\n",
       "      <td>[cannabis, women, norml]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thought for Thursday.‚ú® . . . #words #wordstoli...</td>\n",
       "      <td>[words, wordstoliveby, speakit, power, mommalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.@StatusofWomenNT launched a Taxi Safety Surve...</td>\n",
       "      <td>[nwt, yzf, women, cabbies, yellowknife, nwtpol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L&amp;amp;Z #JewelryRoll Bag | Travel Jewelry Roll...</td>\n",
       "      <td>[jewelryroll, women]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Again, üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª!!! The Marxist freakshow aka...</td>\n",
       "      <td>[women, wakeup]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Clubhouse, a Tiny Audio Chat App, Breaks Throu...</td>\n",
       "      <td>[women]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>@FutureDrDrDua @appyjumpindaze #women with #AD...</td>\n",
       "      <td>[women, adhd, anxiety, healthinequalities, nur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>#LGBTQ #Equality is a #UnifyingIssue for Our #...</td>\n",
       "      <td>[lgbtq, equality, unifyingissue, nation, equal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>@Trust_Failed @ShropshireJos @ElaineFalkner @P...</td>\n",
       "      <td>[women]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  \\\n",
       "0   Celebrating #BlackHistoryMonth  sculptor Eliza...   \n",
       "1   #Cannabis use mitigates symptoms of endometrio...   \n",
       "2   Thought for Thursday.‚ú® . . . #words #wordstoli...   \n",
       "3   .@StatusofWomenNT launched a Taxi Safety Surve...   \n",
       "4   L&amp;Z #JewelryRoll Bag | Travel Jewelry Roll...   \n",
       "..                                                ...   \n",
       "84  Again, üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª!!! The Marxist freakshow aka...   \n",
       "85  Clubhouse, a Tiny Audio Chat App, Breaks Throu...   \n",
       "86  @FutureDrDrDua @appyjumpindaze #women with #AD...   \n",
       "87  #LGBTQ #Equality is a #UnifyingIssue for Our #...   \n",
       "88  @Trust_Failed @ShropshireJos @ElaineFalkner @P...   \n",
       "\n",
       "                                             hashtags  \n",
       "0      [blackhistorymonth, black, women, artist, blm]  \n",
       "1                            [cannabis, women, norml]  \n",
       "2   [words, wordstoliveby, speakit, power, mommalu...  \n",
       "3   [nwt, yzf, women, cabbies, yellowknife, nwtpol...  \n",
       "4                                [jewelryroll, women]  \n",
       "..                                                ...  \n",
       "84                                    [women, wakeup]  \n",
       "85                                            [women]  \n",
       "86  [women, adhd, anxiety, healthinequalities, nur...  \n",
       "87  [lgbtq, equality, unifyingissue, nation, equal...  \n",
       "88                                            [women]  \n",
       "\n",
       "[89 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T19:37:41.944015Z",
     "start_time": "2021-02-18T19:37:41.621592Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/carlosruiz/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T19:37:42.001742Z",
     "start_time": "2021-02-18T19:37:41.945935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed1\n",
      "passed2\n",
      "passed3\n",
      "passed4\n",
      "It seems to me we are in the middle of no man's land with respect to the  following:  Opec production speculation, Mid east crisis and renewed  tensions, US elections and what looks like a slowing economy (?\n",
      "compound: -0.5267, neg: 0.197, neu: 0.68, pos: 0.123, \n",
      "), and no real weather anywhere in the world.\n",
      "compound: -0.296, neg: 0.216, neu: 0.784, pos: 0.0, \n",
      "I think it would be most prudent to play  the markets from a very flat price position and try to day trade more aggressively.\n",
      "compound: 0.0183, neg: 0.103, neu: 0.792, pos: 0.105, \n",
      "I have no intentions of outguessing Mr. Greenspan, the US.\n",
      "compound: -0.296, neg: 0.216, neu: 0.784, pos: 0.0, \n",
      "electorate, the Opec ministers and their new important roles, The Israeli and Palestinian leaders, and somewhat importantly, Mother Nature.\n",
      "compound: 0.4228, neg: 0.0, neu: 0.817, pos: 0.183, \n",
      "Given that, and that we cannot afford to lose any more money, and that Var seems to be a problem, let's be as flat as possible.\n",
      "compound: -0.1134, neg: 0.097, neu: 0.823, pos: 0.081, \n",
      "I'm ok with spread risk  (not front to backs, but commodity spreads).\n",
      "compound: -0.0129, neg: 0.2, neu: 0.679, pos: 0.121, \n",
      "The morning meetings are not inspiring, and I don't have a real feel for  everyone's passion with respect to the markets.\n",
      "compound: 0.5815, neg: 0.095, neu: 0.655, pos: 0.25, \n",
      "As such, I'd like to ask  John N. to run the morning meetings on Mon.\n",
      "compound: 0.3612, neg: 0.0, neu: 0.848, pos: 0.152, \n",
      "and Wed.\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "Thanks.\n",
      "compound: 0.4404, neg: 0.0, neu: 0.0, pos: 1.0, \n",
      "Jeff\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n"
     ]
    }
   ],
   "source": [
    "# below is the sentiment analysis code rewritten for sentence-level analysis\n",
    "# note the new module -- word_tokenize!\n",
    "import nltk\n",
    "\n",
    "import nltk.data\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Next, we initialize VADER so we can use it within our Python script\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# We will also initialize our 'english.pickle' function and give it a short name\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "message_text = '''It seems to me we are in the middle of no man's land with respect to the  following:  Opec production speculation, Mid east crisis and renewed  tensions, US elections and what looks like a slowing economy (?), and no real weather anywhere in the world. I think it would be most prudent to play  the markets from a very flat price position and try to day trade more aggressively. I have no intentions of outguessing Mr. Greenspan, the US. electorate, the Opec ministers and their new important roles, The Israeli and Palestinian leaders, and somewhat importantly, Mother Nature.  Given that, and that we cannot afford to lose any more money, and that Var seems to be a problem, let's be as flat as possible. I'm ok with spread risk  (not front to backs, but commodity spreads). The morning meetings are not inspiring, and I don't have a real feel for  everyone's passion with respect to the markets.  As such, I'd like to ask  John N. to run the morning meetings on Mon. and Wed.  Thanks. Jeff'''\n",
    "print(\"passed3\")\n",
    "# The tokenize method breaks up the paragraph into a list of strings. In this example, note that the tokenizer is confused by the absence of spaces after periods and actually fails to break up sentences in two instances. How might you fix that?\n",
    "\n",
    "sentences = tokenizer.tokenize(message_text)\n",
    "print(\"passed4\")\n",
    "# We add the additional step of iterating through the list of sentences and calculating and printing polarity scores for each one.\n",
    "\n",
    "for sentence in sentences:\n",
    "        print(sentence)\n",
    "        scores = sid.polarity_scores(sentence)\n",
    "        for key in sorted(scores):\n",
    "                print('{0}: {1}, '.format(key, scores[key]), end='')\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
